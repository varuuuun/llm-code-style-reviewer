# LLM Configuration
# Specify which LLM provider to use and its credentials

# Provider can be: openai, gemini, claude, or local
provider: openai

# OpenAI Configuration
openai:
  api_key: "sk-placeholder-your-openai-key-here"
  model: "gpt-4o-mini"
  base_url: null  # Leave null for default, or specify custom endpoint
  temperature: 0.7
  max_tokens: 1000

# Google Gemini Configuration
gemini:
  api_key: "placeholder-your-gemini-key-here"
  model: "gemini-2.0-flash"
  temperature: 0.7
  max_tokens: 1000

# Anthropic Claude Configuration
claude:
  api_key: "sk-ant-placeholder-your-claude-key-here"
  model: "claude-3-5-sonnet-20241022"
  temperature: 0.7
  max_tokens: 1000

# Local LLM Configuration (e.g., Ollama, LM Studio, vLLM)
local:
  base_url: "http://localhost:8000"  # Change to your local server
  model: "llama2"  # or mistral, neural-chat, etc.
  temperature: 0.7
  max_tokens: 1000
